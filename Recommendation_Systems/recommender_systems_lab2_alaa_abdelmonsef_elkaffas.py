# -*- coding: utf-8 -*-
"""recommender_systems_lab2_Alaa_Abdelmonsef_Elkaffas.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1L_fc8gNo1b0QtVvMuy7-l7zxZXETK1C5

$$ ITI \space AI-Pro: \space Intake \space 45 $$
$$ Recommender \space Systems $$
$$ Lab \space no. \space 2 $$

# `01` Import Necessary Libraries

## `i` Default Libraries
"""

!pip install numpy==1.24.3
!pip install scikit-surprise

import numpy as np
import pandas as pd
from surprise.reader import Reader
from surprise.dataset import Dataset
from surprise.model_selection import train_test_split
from surprise.prediction_algorithms.knns import KNNWithMeans
from scipy.sparse import csr_matrix

"""## `ii` Additional Libraries
Add imports for additional libraries you used throughout the notebook
"""

from sklearn.preprocessing import OneHotEncoder, StandardScaler, MinMaxScaler
from sklearn.metrics.pairwise import cosine_similarity

"""----------------------------

# `02` Load Data

 The dataset will have the following columns :
   - song_id (String) : Unique identified for the song
   - user_id (String) : Unique identifier for the user
   - song_genre (Integer) : An integer representing a genre for the song, value is between 1 and 5, indicating that there are 5 unique genres. Each song can only have 1 genre
   - artist_id (String) : Unique identifier for the author of the song
   - n_listen (Integer) : The number of times this user has heard the song (0 -> 15)
   - publish_year (Integer) : The year of song publishing
"""

data = pd.read_csv("/content/songs_data.csv")
data.head()

"""--------------------------

# `03` Content-based Filtering

Practice for content-based filtering on dummy data

## `i` Feature Engineering/Selection
Construct the item vector representation matrix from the `data` above
"""

num_features = ['n_listen', 'publish_year']
cat_feature = 'song_genre'
scaler = MinMaxScaler()
data[num_features] = scaler.fit_transform(data[num_features])
genre_encoder = OneHotEncoder(sparse_output=False)
genre_encoded = pd.DataFrame(
    genre_encoder.fit_transform(data[[cat_feature]]),
    columns=genre_encoder.get_feature_names_out([cat_feature])
)
data = pd.concat([data, genre_encoded], axis=1)
data.head()

"""## `ii` Utility Matrix
Construct utility matrix for the loaded dataframe `data`
"""

utility_matrix = data.pivot_table(index='user_id', columns='song_id', values='n_listen', fill_value=0)
utility_matrix.head()

"""## `iii` Item-Item Similarity Matrix

Construct item-item (Cosine/Adjusted Cosine) similarity matrix.
"""

df_features = data[['song_genre', 'n_listen', 'publish_year']].copy()
df_features = df_features.join(pd.get_dummies(data['song_genre']).astype(int))
scaler = MinMaxScaler()
df_features[['n_listen', 'publish_year']] = scaler.fit_transform(df_features[['n_listen', 'publish_year']])
df_features.drop(columns=['song_genre'], inplace=True)
item_similarity_matrix = cosine_similarity(utility_matrix.T)
item_similarity_df = pd.DataFrame(item_similarity_matrix, index=utility_matrix.columns, columns=utility_matrix.columns)
item_similarity_df.head()

"""## `iv` Top-K Candidate Generation

Selet top-K (a k of your choice) similar items for each item (a user of your choice) rated from the similarity matrix above.
"""

def get_top_k_similar_items(item_id, k=4):
    similar_items = item_similarity_df[item_id].sort_values(ascending=False).iloc[1:k+1]
    return similar_items
item_id = utility_matrix.columns[0]
top_k_similar_items = get_top_k_similar_items(item_id, k=5)
top_k_similar_items

"""## `v` Candidate Filtering

Filter out items (your user) has rated from the candidates above.
"""

def filter_rated_items(user_id, top_k_recommendations, utility_matrix):
    rated_items = utility_matrix.loc[user_id]
    rated_items = rated_items[rated_items > 0].index
    filtered_recommendations = top_k_recommendations[~top_k_recommendations.index.isin(rated_items)]
    return filtered_recommendations
user_id = 2066
filtered_recommendations = filter_rated_items(user_id, top_k_similar_items, utility_matrix)
filtered_recommendations

"""## `vi` Candidate Rating Prediction

Calculate the predicted rating for each of the candidate items.
"""

import numpy as np
import pandas as pd

def predict_ratings(user_id, utility_matrix, similarity_matrix):
    user_ratings = utility_matrix.loc[user_id]
    unrated_items = user_ratings[user_ratings == 0].index
    predicted_ratings = {}
    for item in unrated_items:
        similar_items = similarity_matrix[item]
        rated_items = user_ratings[user_ratings > 0].index
        numerator = np.dot(user_ratings[rated_items], similar_items[rated_items])
        denominator = np.abs(similar_items[rated_items]).sum()
        if denominator > 0:
            predicted_ratings[item] = numerator / denominator
        else:
            predicted_ratings[item] = 0
    return pd.Series(predicted_ratings).sort_values(ascending=False)
user = 2066
predicted_ratings = predict_ratings(user, utility_matrix, item_similarity_df)
predicted_ratings

"""--------------------------

# `04` KNN Item-based Colaborative Filtering

Practice for Using Scikit Surprise Library

## `i` Data Loading

Load `songsDataset.csv` file into a dataframe
"""

df = pd.read_csv('/content/songsDataset.csv')
df.head()

df['rating'].min()

df['rating'].max()

"""## `ii` Prepare Data

Procedures to Follow:
- Instantiate the Reader Object (see, [Documentation](https://surprise.readthedocs.io/en/stable/reader.html))
- Load the Data into `surprise.dataset.Dataset` (see, [Documentation](https://surprise.readthedocs.io/en/stable/dataset.html))
- Build the full (i.e. without folds) `surprise.Trainset` (see, [Documentation](https://surprise.readthedocs.io/en/stable/trainset.html#:~:text=It%20is%20used%20by%20the%20fit()%20method%20of%20every%20prediction%20algorithm.%20You%20should%20not%20try%20to%20build%20such%20an%20object%20on%20your%20own%20but%20rather%20use%20the%20Dataset.folds()%20method%20or%20the%20DatasetAutoFolds.build_full_trainset()%20method.))
"""

reader = Reader(rating_scale=(1, 5))

data = Dataset.load_from_df(df[['userID', 'songID', 'rating']], reader)
data

"""## `iii` Initialize the `KNNWithMeans` Model

**Note**: `KNNWithMeans` uses the normalized ratings instead of the raw ones. (See [Documentation](https://surprise.readthedocs.io/en/stable/knn_inspired.html#surprise.prediction_algorithms.knns.KNNWithMeans))

**Hint**: Use $k=10$ and configure `sim_options` to be:
- item_based
- pearson
"""

sim_options = {
    "name": "pearson",
    "user_based": False
}
knn_model = KNNWithMeans(k=10, sim_options=sim_options)

"""## `iv` Fit the Model on Data"""

trainset = data.build_full_trainset()
knn_model.fit(trainset)

"""## `v` Calculate Predicted Rating $\hat{r}$ for User $199988$

**Hine**: you can use `.predict()` method of the model (see [Documentaion](https://surprise.readthedocs.io/en/stable/getting_started.html?highlight=.predict#train-on-a-whole-trainset-and-the-predict-method:~:text=pred%20%3D%20algo.predict(uid%2C%20iid%2C%20r_ui%3D4%2C%20verbose%3DTrue)))
"""

song_predictions = knn_model.predict(uid=199988, iid=1)
song_predictions

user_id = 199988
predictions = []
for song_id in df['songID'].unique():
    pred = knn_model.predict(user_id, song_id)
    predictions.append((song_id, pred.est))
song_predictions = pd.DataFrame(predictions, columns=["songID", "Predicted Rating"])
song_predictions.head()

"""## `vi` Recommend Top 10 Songs"""

# song_predictions = pd.DataFrame(predictions, columns=["songID", "Predicted Rating"])
song_predictions_sorted = song_predictions.sort_values(by="Predicted Rating", ascending=False)
song_predictions_sorted.head(10)

song_predictions_sorted = None
song_predictions_sorted.head(10)

"""----------------------------------------------

$$ Wish \space you \space all \space the \space best \space â™¡ $$
$$ Mahmoud \space Shawqi $$
"""